{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dweep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dweep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('intents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Find the items in the store.</td>\n",
       "      <td>Object detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Can you identify the objects on the floor?</td>\n",
       "      <td>Object detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>What are the objects in this picture?</td>\n",
       "      <td>Object detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Detect any items in the photo.</td>\n",
       "      <td>Object detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Identify the objects in this snapshot.</td>\n",
       "      <td>Object detection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text            intent\n",
       "92                Find the items in the store.  Object detection\n",
       "93  Can you identify the objects on the floor?  Object detection\n",
       "94       What are the objects in this picture?  Object detection\n",
       "95              Detect any items in the photo.  Object detection\n",
       "96      Identify the objects in this snapshot.  Object detection"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 77/77 [00:00<00:00, 912.04 examples/s]\n",
      "Map: 100%|██████████| 20/20 [00:00<00:00, 809.71 examples/s]\n",
      "c:\\Users\\dweep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode the labels\n",
    "df['label'] = df['intent'].factorize()[0]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Convert Datasets to TensorFlow Format\n",
    "train_tf_dataset = train_dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=lambda x: {key: tf.convert_to_tensor([elem[key] for elem in x]) for key in x[0]}\n",
    ")\n",
    "\n",
    "val_tf_dataset = val_dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=lambda x: {key: tf.convert_to_tensor([elem[key] for elem in x]) for key in x[0]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# Load the TinyBERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('google/bert_uncased_L-2_H-128_A-2', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 16s 511ms/step - loss: 0.6993 - sparse_categorical_accuracy: 0.4416 - val_loss: 0.6744 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.6739 - sparse_categorical_accuracy: 0.5714 - val_loss: 0.6506 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6589 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6259 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6342 - sparse_categorical_accuracy: 0.6234 - val_loss: 0.5980 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.6111 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.5695 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.5381 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5567 - sparse_categorical_accuracy: 0.8182 - val_loss: 0.5030 - val_sparse_categorical_accuracy: 0.9500\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5255 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.4650 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.4942 - sparse_categorical_accuracy: 0.9091 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.4520 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3867 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.4027 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.3505 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 0.3823 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.3181 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 0.3391 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2891 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.3015 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2639 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.2833 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2411 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.2556 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2192 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.2394 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1992 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1812 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.1940 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1647 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.1791 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1500 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2430edba1d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_tf_dataset, validation_data=val_tf_dataset, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('path_to_save_model\\\\tokenizer_config.json',\n",
       " 'path_to_save_model\\\\special_tokens_map.json',\n",
       " 'path_to_save_model\\\\vocab.txt',\n",
       " 'path_to_save_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('path_to_save_model')\n",
    "tokenizer.save_pretrained('path_to_save_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at path_to_save_model were not used when initializing TFBertForSequenceClassification: ['dropout_55']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at path_to_save_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'path_to_save_model'  # Replace with your actual path\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text):\n",
    "    # Preprocess the input text\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Make predictions\n",
    "    outputs = model(inputs)\n",
    "    predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "    \n",
    "    # Map label indices to intent names\n",
    "    label_map = {0: 'Read command', 1: 'Object detection'}\n",
    "    \n",
    "    return label_map[predicted_label], predictions.numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: Read command\n",
      "Prediction scores: [0.58779645 0.41220352]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "new_text = \"see the name of that sign\"\n",
    "predicted_intent, prediction_scores = predict_intent(new_text)\n",
    "\n",
    "print(f\"Predicted intent: {predicted_intent}\")\n",
    "print(f\"Prediction scores: {prediction_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
